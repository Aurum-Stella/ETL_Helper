# Облегчатор ETL/LTE, и прочей рутины.

Стараюсь тут облегчить рутину и реализовать функционал некоторых  
рабочих задач которые выполняются не ефективно либо вообще вручную:
## ETL
### E - extract
1. Удобное взаимодействие с разными бд, а именно, передача данных для запросов и записи между бд  
используя одновременно пулы подключений для разных бд в библиотеке `asyncpg`;
2. Использование временных таблиц в промежуточных запросах/вычислениях  
(конечно это вообще не инновация, но мы почему то этого не делали);
3. Авто подставление плейсхолдеров в запросы для выгрузки данных по нескольким параметрам  
например получить данные за периоды с интервалом в месяц начиная с 2023-01-01  
и записать каждый из них в свой файл, или передать далее для обработки;
### T - transform
По правде говоря тут еще ничего не готово но есть определенные идеи:
1. В ход пойдет библиотека `pandas`;
2. Очень заинтересовала библиотека `pandasql` которая позволяет манипулировать датафреймами
с помощью `SQL` запросов. Возможно это будет удобно.
3. Ну и по возможности обработка должна быть асинхронной.
По большей части тут особо упростить жизнь не выйдет, так как трансформация данних довольно ситуативная штука.
### L - load
Тут я просто усовершенствовал несколько методов библиотеки `asyncpg`, что бы их было удобно использовать:
1. Копирование результата запроса в вайл/ы csv;
2. Копирование записей (records) в таблицу;
3. Копирование данных в таблицу из файла;

### Миграции
Если честно, немного надоело хранить конфигурации таблиц где попало, и по этому решил использовать к `SQLAlchemy`  
и его возможностей ORM для создания и управления таблицами как обьектами Python и хранить их вместе с проэктом.
А так же `Alembic` - библиотека для Алхимии для миграций.
1. Теперь можно создать таблицу как класс пайтон, что намного удобней чем писать `SQL` запрос.
2. При изменении данных в классе таблицы (типы данных, добавление/удаление столбцов, связи, индексы)  
достаточно создать новую миграцию, и изменения вступят в силу в БД, а по коду пайтон намного легче понять  
схему таблицы, связи и тд.

Конечно это не панацея, и некоторые вещи придется писать самостоятельно. Но идея была такова что бы, где не нужно,  
Python кода было меньше, и он был шаблонным/блочным и неизменяем. Задача пользователя будет только в том что бы написать  
нужный `SQL` запрос, и передать его в нужный шаблонный метод, результат передать в другой метод и тд.

## По структуре:
#### Каталог `src` содержит всякую всячину, настройки, конфиги подключений к бд, методы и тд - сердце проэкта:
##### Подкаталог `config` содержит конфигурации по подключению и работе с AWS_Lambda.  По большей части там менять ничего не нужно:
1. `src/config/aws_settings.py` - настройки для AWS-Lambda. Они - нужны, зачем - не знаю.
2. `src/config/connected.py` - содержит класс Settings который создает нужное подключение к одной из БД.
##### Подкаталог `etl` содержит файлы для обработки данных. 
1. `src/etl/sql` - тут создаем пользовательские `SQL` запроси в формате `.sql` которые будут использованы в прэкте.
2. `src/etl/extract.py` - тут ничего не трогаем. Содержит в себе уже готовые методы для исполнения запросов,   
базовые методы получения и записи данных.
3. `src/etl/transform.py` - тут ничего нет. Сюда пишем код для трансформинга и подсчетов данных.  
`transform.py` не должен быть исполняемым, а должен вызываться как модуль который все сделает, и вернет готовый результат.

### Модели и миграции.
#### Все модели и миграции делаем отдельно от остального проэкта, то есть все что происходит в каталогах ниже не нужно никуда  импортировать. Создание таблиц и изменение их структуры это одно, а etl это совершенно другое. 
##### Подкаталог `migrations` содержит файлы миграций и их конфиги.
1. `src/etl/versions` - каталог который содержит инфу о том, что произойдет после применении миграции, ну и собственно все  
примененный миграции по проэкту. Обязательно проверять что там указано перед применением миграции;
2. Остальные файлы в этом каталоге это конфиги миграций и тд. Об этом лучше почитать в документации `Alembic`;
##### Подкаталог `models` содержит файлы миграций и их конфиги.
1. `src/etl/migrations` - тут создаем модели - обьекты таблиц, связи, вносим изменения, после чего применяем миграции;

##### Подкаталог `utils` содержит файлы c пользовательскими методами.
1. `src/etl/utils/csv` - тут будут созданы csv файлы с данными при вызове нужного метода.
2. `src/etl/utils/custom_type.py` - тут лежат пользовательские типы данных для моделей таблиц.
3. `src/etl/utils/utils.py` - тут лежат пользовательские методы для облегчения жизни. 

# Ниже, когда-то, появиться описание того как этим всем пользоваться. А пока там будет шаблоны кода.


* Асинхронно выполнить запросы по периодам:
```python
tasks = [self.extract(sql_file='test.sql', 
                      placeholder=placeholder, 
                      pool=self.facade_pool).run_sql_query_async() for placeholder in self.periods]
await asyncio.gather(*tasks)
```

* Асинхронно выполнить запросы по периодам и записать в файлы:
* * date: bool - параметр записывать ли периоды в название файла
```python
tasks = [self.extract(sql_file='test.sql',
                      placeholder=placeholder,
                      pool=self.facade_pool).run_sql_query_and_write_to_file(date=True, number=n) for
         placeholder, n in
         zip(self.periods, range(len(self.periods)))]
await asyncio.gather(*tasks)
```

* Последовательно выполнить запросы по периодам и записать в файлы:
* * write_to_csv нужно изменить `return list_of_periods` на `return periods` 
и в запросе использовать именованые аргументы `%(start_period)s` и `%(end_period)s`
```python
data = []

[data.append(self.extract(sql_file='test.sql',
                          placeholder=period, 
                          connected=connected).run_sql_query_default())
for period in self.periods]
write_data_to_csv = WriteDataToFile(data=data,
                                    parameters=self.periods)
write_data_to_csv.write_to_csv()
```


# В планах:
- [x] Вообще начать что ли бы делать по этому поводу;
- [x] Завести гит под этот проэкт;
- [ ] Дописать редми(исполняемый файл);
- [ ] Дописать редми(.env);
- [ ] Дописать редми(первый запуск);
- [ ] Дописать редми(методы и их использование);
- [ ] Разобраться с `Pandasql`;
- [ ] Посмотреть что там по асинхронности у MySql;
- [ ] Попробовать наконец-то сделать миграции не на тестовой базе (боюсь все дропнуть);
- [x] Плакать, потому что кроме меня этим пользоваться скорее всего никто не будет;
